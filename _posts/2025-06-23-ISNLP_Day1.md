---
title: "[ISNLP 오픈 튜토리얼] 1일차: 선형 회귀와 로지스틱 회귀 실습"
date: 2025-06-23 21:00:00 +09:00
categories: [AI, Study]
tags: [Linear Regression, Logistic Regression, PyTorch, ISNLP, Machine Learning]
math: true
toc: true
pin: true
image:
  path: /assets/img/for_post/ISNLP_Day1/thumbnail.jpg
  alt: ISNLP 1일차: 선형 회귀와 로지스틱 회귀
---

 **[ISNLP 오픈 튜토리얼] 1일차: 선형 회귀 & 로지스틱 회귀**  

이번 1일차 수업에서는 머신러닝의 기초를 다지는 **선형 회귀(Linear Regression)**와  
**로지스틱 회귀(Logistic Regression)**를 학습하고, Python과 PyTorch로 직접 구현해보았습니다.

---

## **1. 오늘의 학습 주제**
- **Linear Regression**
  - 1차 함수 기반 예측 모델
  - MSE(Mean Squared Error) 손실 함수
  - 최소제곱법(Least Squares) & 경사 하강법(Gradient Descent) 학습
- **Logistic Regression**
  - 0/1 이진 분류 문제
  - Sigmoid 함수를 활용한 확률 출력
  - Binary Cross-Entropy 손실 함수
  - Sympy와 PyTorch를 이용한 구현 실습

---

## **2. Linear Regression 실습**

#### (1) 핵심 개념
- 데이터를 가장 잘 표현하는 **직선**을 찾는 문제
- 손실 함수: 
\[
MSE = \frac{1}{n}\sum_i (\hat{y_i} - y_i)^2
\]
- 최적화 방법
  1. **최소제곱법(Closed-form)**
  2. **경사 하강법(Gradient Descent)**

#### (2) Python 예시
```py
# 1-1.linear_regression_with_LeastSquares.ipynb
import numpy as np

x = np.array([10, 15, 20, 25])
y = np.array([1, 2, 3, 4])

a = np.sum((x - x.mean())*(y - y.mean())) / np.sum((x-x.mean())**2)
b = y.mean() - a*x.mean()
print(f"직선 방정식: y = {a:.2f}x + {b:.2f}")
```

#### (3) Gradient Descent 예시
```py
# 1-2.linear_regression_with_GradientDescent.ipynb
learning_rate = 0.01
a, b = 0.0, 0.0

for epoch in range(100):
    y_pred = a*x + b
    grad_a = (-2/len(x)) * sum(x * (y - y_pred))
    grad_b = (-2/len(x)) * sum(y - y_pred)

    a -= learning_rate * grad_a
    b -= learning_rate * grad_b
```

---

## **3. Logistic Regression 실습**

#### (1) 핵심 개념
- 선형 회귀 결과를 **Sigmoid 함수**로 확률화
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]
- 손실 함수: **Binary Cross-Entropy**
\[
L = -\frac{1}{n}\sum_i [y_i \log \hat{y_i} + (1-y_i)\log(1-\hat{y_i})]
\]

#### (2) PyTorch 구현
```py
# 2-2.logistic_regression_with_backpropagation(pytorch).ipynb
import torch

X = torch.tensor([[25, 12, 40]], dtype=torch.float32)
y = torch.tensor([1], dtype=torch.float32)

w = torch.randn((3,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

sigmoid = torch.nn.Sigmoid()
loss_fn = torch.nn.BCELoss()
optimizer = torch.optim.SGD([w,b], lr=0.01)

for epoch in range(100):
    y_pred = sigmoid(X @ w + b)
    loss = loss_fn(y_pred, y.unsqueeze(1))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## **4. 오늘의 배움과 느낀 점**
- **중요하게 본 내용**
  - 선형 회귀와 로지스틱 회귀의 핵심 차이는 **출력 값(연속/확률)과 손실 함수**에서 발생
  - 경사 하강법을 통해 기울기와 학습률의 의미를 직접 체감

- **배운 점**
  - 수식을 코드로 구현하면서 `수식 → 미분 → 파라미터 업데이트` 흐름을 이해
  - PyTorch 기본 학습 구조(`requires_grad`, `backward()`, `optimizer.step()`) 습득

- **느낀 점**
  - 이전까지 케라스/텐서플로우 위주였는데, 이번 실습으로 **PyTorch**에 익숙해질 수 있었음
  - 알고 있다고 생각했던 개념도 직접 구현하며 복습할 수 있었고  
    **기초를 다시 탄탄하게 다질 필요성**을 느낀 하루였음

---

## **5. 참고 자료**
- [WikiDocs: Linear Regression](https://wikidocs.net/21670)
- [WikiDocs: Logistic Regression](https://wikidocs.net/22881)

---

다음 포스팅에서는 **2일차 실습: MLP와 딥러닝 기초**를 다룰 예정입니다.
