---
title: "[ISNLP 오픈 튜토리얼] 8일차: Pre-trained Model과 Hugging Face"
date: 2025-07-08 21:00:00 +09:00
categories: [ISNLP_Open_Tutorial]
tags: [Pretrained Model, BERT, GPT, T5, HuggingFace, NLP]
math: true
toc: true
pin: true
image:
  path: /assets/img/for_post/ISNLP_Day8/huggingface.png
  alt: "ISNLP 8일차: Pre-trained Model과 Hugging Face"
---

# 8일차: Pre-trained Model과 Hugging Face

이번 8일차 수업에서는 **사전학습(Pre-train)된 모델**과  
**Hugging Face 생태계**를 중심으로 최신 NLP 모델 활용법을 배웠다.  

BERT, GPT, T5, BART와 같은 최신 Transformer 계열 모델과  
Hugging Face 라이브러리를 배우면서  
앞으로의 학습·프로젝트 방향을 구체적으로 정할 수 있었다.

---

## 1. Pre-trained Model & Transfer Learning

### 1-1. 사전학습(Pre-train)
- 대규모 코퍼스에서 정답 없이 학습
- 자연어 일반적 특징(문법, 의미)을 학습
- 대표적인 Pre-train Task
  1. **Masked Language Modeling (MLM)**  
     - 입력 문장의 일부 토큰을 `[MASK]`로 가리고 예측 (BERT, RoBERTa 등)
  2. **Next Token Prediction (Causal LM)**  
     - 이전 단어를 기반으로 다음 단어 예측 (GPT 계열)

---

### 1-2. 전이학습(Fine-tuning)
- Pre-trained Model + Task-specific Head
- 목적 Task 데이터셋으로 미세 조정(Fine-tuning)
- 대표 모델 구조
  - **Encoder 기반**: BERT → 문장 분류, NER, QA
  - **Decoder 기반**: GPT → 텍스트 생성
  - **Encoder-Decoder 기반**: T5, BART → 번역, 요약

---

### 1-3. Pre-trained Model의 장점
- 데이터와 시간 절약: 대규모 코퍼스에서 학습된 지식 활용
- 다양한 NLP Task에 쉽게 적용 가능
- Hugging Face를 통한 모델 공유 및 재사용 용이

---

## 2. Hugging Face 활용

### 2-1. 모델 로드
```py
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
```

- `from_pretrained()`으로 사전학습된 모델과 토크나이저 즉시 로드 가능

---

### 2-2. 번역 예시 (T5)
```py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

text = "translate English to French: I love NLP."
input_ids = tokenizer(text, return_tensors="pt").input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

### 2-3. 요약 예시 (BART)
```py
from transformers import BartTokenizer, BartForConditionalGeneration

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")

article = "Natural Language Processing is a subfield of AI..."
inputs = tokenizer(article, return_tensors="pt", max_length=1024, truncation=True)
summary_ids = model.generate(inputs.input_ids, max_length=50, min_length=10)
print(tokenizer.decode(summary_ids[0], skip_special_tokens=True))
```

---

## 3. 오늘 배운 내용 정리

### 3-1. 중요하게 본 내용
- 최신 NLP 모델(BERT, GPT, T5, BART)의 구조와 활용
- Hugging Face를 통한 모델 로드, 추론, Fine-tuning 방법
- Pre-trained Model을 활용한 Transfer Learning의 핵심

### 3-2. 배운 점
- Pre-train → Fine-tuning 과정을 통해 데이터와 시간 효율을 높일 수 있음을 이해
- Hugging Face에서 수많은 공개 모델을 쉽게 활용 가능
- 번역, 요약, 감정분류 등 다양한 NLP Task를 실제로 구현 가능

### 3-3. 느낀 점
- 최신 GPT, BERT, T5 모델과 Hugging Face를 배우며 앞으로의 학습 방향이 명확해졌다
- 허깅페이스를 적극 활용해 공부와 포트폴리오 프로젝트를 진행해보고 싶어졌다
- 언젠가는 **직접 학습한 모델을 Hugging Face에 업로드**해보고 싶다는 목표가 생겼다

---

## 4. 참고 자료
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Pre-trained Models 개념 정리](https://wikidocs.net/166759)
- [BERT 논문](https://arxiv.org/abs/1810.04805), [GPT 논문](https://arxiv.org/abs/2005.14165)
