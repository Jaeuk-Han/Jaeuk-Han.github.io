---
title: "[AI ë§í‰ ëŒ€íšŒ] ì°¸ì—¬ê¸° #10: 3ì£¼ì°¨(2) - GRPO ê¸°ë°˜ ê°•í™”í•™ìŠµê³¼ DeepSeekì˜ ì‚¬ë¡€"
date: 2025-07-23 22:00:00 +09:00
categories: [2025_ë§í‰]
tags: [ë§í‰ëŒ€íšŒ, Reinforcement-Learning, GRPO, RLHF, RAG, DeepSeek, ê°•í™”í•™ìŠµ]
toc: true
pin: true
math: true
image:
  path: /assets/img/for_post/MalPyeong/week3_rl.png
  alt: "AI ë§í‰ ëŒ€íšŒ GRPO ê°•í™”í•™ìŠµ"
---

# AI ë§í‰ ëŒ€íšŒ ì°¸ì—¬ê¸° #10: 3ì£¼ì°¨(2) - GRPO ê¸°ë°˜ ê°•í™”í•™ìŠµê³¼ DeepSeek ì‚¬ë¡€

ì§€ë‚œ ê¸€ì—ì„œëŠ” ë“€ì–¼ ì¸ì½”ë”ê°€ êµ¬ì¡°ì ìœ¼ë¡œ í•œê³„ê°€ ìˆìŒì„ í™•ì¸í–ˆê³ , ìš°ë¦¬ íŒ€ì€ **PPL ê¸°ë°˜ candidate + ê°•í™”í•™ìŠµ(RL)**ìœ¼ë¡œ gold ë¬¸ë§¥ì„ ì„ íƒí•˜ëŠ” ì „ëµì„ ì±„íƒí–ˆë‹¤.  

ì´ë²ˆ ê¸€ì—ì„œëŠ” ë‚´ê°€ ë§¡ì€ GRPO íŠ¸ë ˆì´ë„ˆ íŒŒíŠ¸ì— ëŒ€í•´ ê³µë¶€í•œ ë‚´ìš©ì„ ì •ë¦¬í•œë‹¤.

ê¸€ì—ì„œëŠ” í¬ê²Œ ê°•í™”í•™ìŠµ ê°œë…ë“¤ê³¼ **GRPO (Group Relative Policy Optimization)** ì•Œê³ ë¦¬ì¦˜ ê·¸ë¦¬ê³  ì´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì ìš©í•œ **DeepSeekì˜ ì‚¬ë¡€**ë¥¼ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤.

---

## 1) ê°•í™”í•™ìŠµ(RL) ê¸°ì´ˆ ê°œë…

ê°•í™”í•™ìŠµì€ í¬ê²Œ ë„¤ ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤:

- **í™˜ê²½(Environment)**: ëª¨ë¸ì´ ìƒí˜¸ì‘ìš©í•˜ëŠ” ëŒ€ìƒ (ìš°ë¦¬ ê³¼ì œì—ì„œëŠ” í”„ë¡¬í”„íŠ¸)  
- **ì—ì´ì „íŠ¸(Agent)**: í–‰ë™ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ (LLM)  
- **í–‰ë™(Action)**: ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ì‘ë‹µ (í…ìŠ¤íŠ¸ ì¶œë ¥)  
- **ë³´ìƒ(Reward)**: ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì‹ í˜¸ (í¬ë§· ì¤€ìˆ˜ ì—¬ë¶€ ë“±)

LLM ê°•í™”í•™ìŠµì—ì„œ ìì£¼ ì“°ì´ëŠ” í˜•íƒœëŠ” **RLHF (Reinforcement Learning with Human Feedback)**ë‹¤.  
RLHFì˜ ì „í˜•ì  êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

1. **Supervised Fine-Tuning (SFT)**: ê¸°ë³¸ ì‘ë‹µ ë°ì´í„°ë¡œ ì´ˆê¸° í•™ìŠµ  
2. **Reward Model (RM)**: ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ëª¨ë¸ í•™ìŠµ  
3. **PPO (Proximal Policy Optimization)**: SFT ëª¨ë¸ì„ RM ë³´ìƒì— ë”°ë¼ ì¡°ì •

í•˜ì§€ë§Œ PPOëŠ” critic ëª¨ë¸ê³¼ RM í•™ìŠµì´ í•„ìš”í•´ **ë³µì¡í•˜ê³  ìì› ì†Œëª¨ì **ì´ë‹¤.

---

## 2) GRPO: PPOì˜ ëŒ€ì•ˆ

**GRPO (Group Relative Policy Optimization)**ëŠ” PPOì˜ í•œê³„ë¥¼ ê°œì„ í•œ ë°©ì‹ì´ë‹¤.

- **Value ëª¨ë¸ì´ í•„ìˆ˜ê°€ ì•„ë‹˜** â†’ critic ì—†ì´ í•™ìŠµ ê°€ëŠ¥  
- **ê·¸ë£¹ ë‚´ ìƒëŒ€ì  ë¹„êµ** â†’ ì—¬ëŸ¬ ì‘ë‹µì„ ìƒì„±í•´ ë³´ìƒì„ ìƒëŒ€ ë¹„êµ  
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì **ì´ê³ , **ì•ˆì •ì  í•™ìŠµ** ê°€ëŠ¥

ê³µì‹ ì •ì˜:  
> í•œ í”„ë¡¬í”„íŠ¸ì—ì„œ ì—¬ëŸ¬ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ê·¸ ì¤‘ ìƒëŒ€ì ìœ¼ë¡œ ë” ë‚˜ì€ ì‘ë‹µì´ ì–´ë–¤ ê²ƒì¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•œë‹¤.

---

## 2-1) DeepSeek ì‚¬ë¡€

ìµœê·¼ ë°œí‘œëœ **DeepSeekMath** ì—°êµ¬ì—ì„œëŠ” GRPOë¥¼ í™œìš©í•´ **GPT-4 ìˆ˜ì¤€ì— ê·¼ì ‘í•œ ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥**ì„ í™•ë³´í–ˆë‹¤.

- DeepSeekMathâ€‘Instruct 7B ëª¨ë¸ì€ **MATH benchmarkì—ì„œ 51.7%** ì„±ëŠ¥ì„ ë‹¬ì„± â†’ GPT-4ì™€ ê·¼ì ‘ ([Zhang et al., 2024](https://arxiv.org/abs/2402.03300))  
- GRPO ë•ë¶„ì— **critic ëª¨ë¸ì´ í•„ìš” ì—†ì—ˆê³ , ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ** ê°€ëŠ¥  
- ë‹¨ìˆœ SFT ëª¨ë¸ì—ì„œ GRPO RLì„ ê±°ì¹œ ë’¤ **reasoning ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ**ë¨

ë˜í•œ DeepSeek-R1â€‘Zero ì‹¤í—˜ì—ì„œëŠ” **SFT ì—†ì´ë„ GRPOë§Œìœ¼ë¡œ ëª¨ë¸ì˜ reasoning ëŠ¥ë ¥ì´ ë°œì „**í•˜ëŠ” ì‚¬ë¡€ê°€ ë³´ê³ ë˜ì—ˆë‹¤.  
ì´ëŠ” ê°•í™”í•™ìŠµ ìì²´ê°€ ëª¨ë¸ì˜ ìê¸° ì§„í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤ëŠ” ì ì„ ì‹œì‚¬í•œë‹¤.

---

## 3) Hugging Face GRPOTrainer ì‚¬ìš©ë²•

Hugging Face TRL ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œëŠ” GRPOTrainerë¥¼ ì œê³µí•œë‹¤.

```python
from datasets import load_dataset
from trl import GRPOConfig, GRPOTrainer

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("your_eval_prompts", split="train")

# ë³´ìƒ í•¨ìˆ˜ ì •ì˜ (ì˜ˆì‹œ)
def reward_format(completions, **kwargs):
    scores = []
    for c in completions:
        if c.strip().startswith("ì •ë‹µ") and "ì˜³ë‹¤." in c:
            scores.append(1.0)
        else:
            scores.append(0.0)
    return scores

# ì„¤ì •
training_args = GRPOConfig(
    output_dir="malpyeong-grpo",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    num_generations=4,   # í•œ í”„ë¡¬í”„íŠ¸ ë‹¹ ìƒì„±í•  ì‘ë‹µ ìˆ˜
    learning_rate=1e-5,
    gradient_accumulation_steps=2,
    logging_steps=20,
)

# í•™ìŠµ ì‹¤í–‰
trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=[reward_format],
    train_dataset=dataset,
    args=training_args,
)

trainer.train()
```

### ì£¼ìš” ì¸ì ì„¤ëª…
- **num_generations**: í•œ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±í•  ì‘ë‹µ ìˆ˜ (ê·¸ë£¹ í¬ê¸°)  
- **reward_funcs**: ë³´ìƒ í•¨ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µ ê°€ëŠ¥ (ì˜ˆ: í¬ë§· ë³´ìƒ + ë‚´ìš© ë³´ìƒ)  
- **per_device_train_batch_size**: ë°°ì¹˜ í¬ê¸°  
- **gradient_accumulation_steps**: GPU ë©”ëª¨ë¦¬ ìµœì í™”  
- **logging_steps** / **save_steps**: í•™ìŠµ ëª¨ë‹ˆí„°ë§

---

## 4) ìš°ë¦¬ íŒ€ì˜ ë³´ìƒ ì„¤ê³„ ì•„ì´ë””ì–´

ê°•í™” í•™ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” **ë³´ìƒì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì¤„ì§€**ê°€ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤.

ìš°ë¦¬ëŠ” ì‹¤ì œ ëŒ€íšŒ í‰ê°€ ì§€í‘œë“¤ê³¼ ì •í•´ì§„ í˜•ì‹ì„ ì§€í‚¤ëŠ”ì§€ë¥¼ ë³´ìƒìœ¼ë¡œ ì‚¼ìœ¼ë©´ í•©ë¦¬ì ì¼ ê²ƒì´ë¼ íŒë‹¨í–ˆë‹¤.

- **Format Reward**:  
  ì¶œë ¥ì´ ë°˜ë“œì‹œ `"{ì„ íƒÂ·êµì • ë¬¸ì¥}ì´/ê°€ ì˜³ë‹¤. {ì´ìœ }"` í˜•íƒœë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸  

- **ëŒ€íšŒ í‰ê°€ ì§€í‘œ ê¸°ë°˜ Reward**:  
  - Exact Match (EM)  
  - ROUGE-1  
  - BERTScore  
  - BLEURT  


---

## 5) ì •ë¦¬ ë° ë‹¤ìŒ ê³„íš

- GRPOëŠ” PPO ëŒ€ë¹„ **ë‹¨ìˆœí•˜ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê°•í™”í•™ìŠµ ë°©ë²•**  
- DeepSeek ì‚¬ë¡€ëŠ” GRPOê°€ ì‹¤ì œë¡œ reasoning ëŠ¥ë ¥ ê°•í™”ë¥¼ ê°€ëŠ¥ì¼€ í•¨ì„ ë³´ì—¬ì¤Œ  
- ìš°ë¦¬ íŒ€ì€ ëŒ€íšŒ í‰ê°€ ì§€í‘œ ê¸°ë°˜ ë³´ìƒ + Format rewardë¥¼ ì¡°í•©í•˜ëŠ” ì „ëµì„ ê³„íš ì¤‘ì„  

ğŸ‘‰ ë‹¤ìŒ ê¸€(#11)ì—ì„œëŠ” **ìš°ë¦¬ íŒ€ì˜ ì‹¤ì œ GRPO êµ¬í˜„ ì½”ë“œ**ì™€ **í•™ìŠµ ê³¡ì„  ë° ê²°ê³¼**ë¥¼ ì •ë¦¬í•œë‹¤.
